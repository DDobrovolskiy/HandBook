link: https://github.com/ArtashesAvetisyan/sbercode-scenarios/tree/master/sc2

### Введение
В данном упражнении будет осуществлено канареечное развертывание сервисов, плавная смена направления трафика внутри service mesh, открытие исходящего трафика с соединением с API сервиса в другом виртуальном кластере

Будут рассмотрены сценарии:

* Установка трех сервисов: ServiceA, ServiceB, ServiceC.
* Настройка маршрутизации входящего трафика service mesh в ServiceA
* Направление исходящих запросов из ServiceA в ServiceB.
* Расщепление трафика и направление запросов из ServiceA, как в ServiceB так и в ServiceC.
* Открытие исходящего трафика из service mesh для получения ответов на запросы ServiceC из прикладного сервиса, развернутом в другом виртуальном кластере.
* Перевод 100% запросов из ServiceA в ServiceC.

### Подготовка среды выполнения
До начала выполнения упражнения давайте подготови необходимую среду выполнения

Запуск виртуальной машины
Перед началом запуска упражнения была развернута виртуальные машина, на которой был установлен Kubernetes и организован минимальный кластер из одного управляющего узла (control-plane), где был установлен Istio.

Давайте запустим Kubernetes и Istio виполнив скрипт:
``` shell
launch.sh
```
До перехода к следующему шагу необходимо убедиться, что узел Kubernetes (node) находятся в состоянии Ready.

Для получения статуса, давайте выполним команду:
``` shell
kubectl get nodes
``` 
Необходимое состояние узла для перехода к следующему шагу приведено ниже:
``` shell
NAME              STATUS   ROLES                  AGE     VERSION
*********         Ready    control-plane,master   8m54s   v1.21.2+k3s1
```
Если вы не наблюдаете подобного вывода, подождите 1-2 минуты и повторите попытку.

Также убедимся, что все существующие поды всех пространстве имен запущены и функционируют корректно: статус (STATUS) - Running, число рестартов (RESTARTS) - 0.

Выполним команду:
``` shell
kubectl get pods --all-namespaces
```
Дождитесь перехода статусов подов до Running.

Конфигурация Istio
Выполним команду:
``` shell
istioctl -c /etc/rancher/k3s/k3s.yaml install -y --set meshConfig.accessLogFile=/dev/stdout --set meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY --set values.pilot.resources.requests.memory=128Mi --set values.pilot.resources.requests.cpu=50m --set values.global.proxy.resources.requests.cpu=10m --set values.global.proxy.resources.requests.memory=32Mi  --set values.global.proxy.resources.limits.memory=64Mi --set values.pilot.resources.limits.memory=256Mi
```
В случае успеха, в выводе вышеприведенной команды должны быть строки:

✔ Istio core installed                                                        
✔ Istiod installed                                                            
✔ Ingress gateways installed                                                  
✔ Installation complete
Создание и конфигурация пространства имен
Для создания нового пространства имен dev-service-mesh выполним команду:
``` shell
kubectl create namespace dev-service-mesh
```
Внесем новоепространство в контекс Kubernetes:
``` shell
kubectl config set-context --current --namespace=dev-service-mesh
``` 
Активируем автоматическое внедрение контейнера с прокси-сервером Envoy в каждый создаваемый под в dev-service-mesh:
``` shell
kubectl label namespace dev-service-mesh istio-injection=enabled
```
Доступ к ingress-шлюзу Istio
Выполним команду:
``` shell
kubectl get svc istio-ingressgateway -n istio-system
```
Экспортируем IP-адрес из этого ресурса в переменную:
``` shell
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
``` 
То же самое сделаем для номера порта ingress-шлюза:
``` shell
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
```
Создадим переменную, содержащую извлеченные ранее данные:
``` shell
export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
``` 
Таким образом, переменная GATEWAY_URL содержит адрес по которому можно совершить запрос в ingress-шлюз:
``` shell
echo $GATEWAY_URL
```

### Сценарии упражнения
Цель данного упражнения заключается в исполнении следующего сценария:

* Установка трех сервисов: ServiceA, ServiceB, ServiceC.
* Настройка маршрутизации входящего трафика service mesh в ServiceA
* Направление исходящих запросов из ServiceA в ServiceB.
* Расщепление трафика и направление запросов из ServiceA, как в ServiceB так и в ServiceC.
* Открытие исходящего трафика из service mesh для получения ответов на запросы ServiceC. В качесте постащика данных будет использован другой прикладной сервис, развернутый в изолированном виртульном кластере (пространстве имен или namespace).
* Перевод 100% запросов из ServiceA в ServiceC.

ServiceA при получении запроса на адрес http://localhost:8081/, для формирования ответа запрашивает информацию у некого поставщика по константному адресу http://producer-internal-host:80/, получив ответ, ServiceA включает его в ответ на вызов из-вне кластера и возвращает его. В качестве подобных поставщиков вступают ServiceB, который всегда возвращает ответ "Hello from ServiceB", и ServiceC, который получив запрос, в свою очередь, совершает запрос в изолированном пространсвте имен на URL http://istio-ingressgateway.istio-system.svc.cluster.local/service-ext и возвращает полученный ответ. ServiceB и ServiceC ожидают запросы на адрес http://localhost:8082/.

Все изменения в направлениях маршрутизации будут происходит исключительно конфигурацией service mesh при помощи API Istio и прозрачно для установленных сервисов.

Исходный код приложений:

https://github.com/ArtashesAvetisyan/sbercode-scenarios/tree/master/apps
Перейдем к следующему шагу.

### Маршрутизация входящего трафика в ServiceA
На этом шаге мы настроим service mesh согласно следующей схеме:
![schema](https://github.com/ArtashesAvetisyan/sbercode-scenarios/blob/master/assets/sc2-1.png?raw=true)
Mesh configuration

Давайте установим ServiceA:
``` shell
kubectl apply -f serviceA-v1-deployment.yml
```
``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-a-v1-deployment
  labels:
    app: service-a-app
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-a-app
      version: v1
  template:
    metadata:
      labels:
        app: service-a-app
        version: v1
    spec:
      containers:
        - name: service-a-container
          image: artashesavetisyan/istio-basics-course:service-a-java-11
          imagePullPolicy: Always
          ports:
            - containerPort: 8081
          securityContext:
            runAsUser: 1000
          resources:
            limits:
              memory: 32Mi
              cpu: 100m
```
Применим Service для деплоймента выше:
``` shell
kubectl apply -f serviceA-srv.yml
```
``` yaml
apiVersion: v1
kind: Service
metadata:
  name: service-a-srv
  labels:
    app: service-a-app
    service: service-a-app-srv
spec:
  ports:
    - port: 80
      name: http-80
      targetPort: 8081
  selector:
    app: service-a-app
```
Создадим Gateway:
``` shell
kubectl apply -f serviceA-gw.yml
```
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: service-a-gw
spec:
  selector:
    istio: ingressgateway
  servers:
    - port:
        number: 80
        name: http
        protocol: HTTP
      hosts:
        - "*"
```
Определим правило маршрутизации:
``` shell
kubectl apply -f inbound-to-serviceA-vs.yml
```
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: inbound-to-service-a-vs
spec:
  hosts:
    - "*"
  gateways:
    - service-a-gw
  http:
    - match:
        - uri:
            exact: /service-a
      rewrite:
        uri: /
      route:
        - destination:
            host: service-a-srv
            port:
              number: 80
```
Подробно тип манифестов выше рассмотрены в упражнении: Конфигурация окружения и запуск прикладного сервиса в sevice mesh

Проверим готовность подов:
``` shell
kubectl get pods --all-namespaces
```
Все поды должны иметь статус Running, дождитесь нужного статсуса.

И наконец совершим GET запрос по адресу ingress-шлюза:
``` shell
curl -v http://$GATEWAY_URL/service-a
``` 
В ответ на совершенный вызов на данном шаге мы должны видеть сообщение: Hello from ServiceA! Calling master system API... I/O error on GET request for "http://producer-internal-host": producer-internal-host; nested exception is java.net.UnknownHostException: producer-internal-host

Что произошло?

Мы совершили запрос в ingress-шлюз, который был перенаправлен в envoy-прокси пода с контейнером ServiceA. Далее запрос был маршрутизирован непосредственно в приложение ServiceA.

ServiceA, получив запрос, совершил запрос по адресу http://producer-internal-host:80/, однако данного хоста еще нет в service mesh, поэтому произошло исключение java.net.UnknownHostException. ServiceA подготовил ответ на внешний вызов и вернул его.

Проверим логи доступа Envoy ingress-шлюза:
``` shell
kubectl logs -l app=istio-ingressgateway -n istio-system -c istio-proxy
```
Проверим логи доступа Envoy в поде с бизнес сервисом:
``` shell
kubectl logs -l app=service-a-app -c istio-proxy
``` 
Перейдем далее.

### Направление исходящих запросов из ServiceA в ServiceB.
На этом шаге мы направим исходящие запросы из ServiceA в ServiceB. На схеме это выглядет слудующим образом:
![sv2](https://github.com/ArtashesAvetisyan/sbercode-scenarios/blob/master/assets/sc2-2.png?raw=true)
Mesh configuration

Давайте установим ServiceB:
``` shell
kubectl apply -f service-b-deployment.yml
```
``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-b-deployment
  labels:
    app: service-b-app
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-b-app
      version: v1
  template:
    metadata:
      labels:
        app: service-b-app
        version: v1
    spec:
      containers:
        - name: service-b-container
          image: artashesavetisyan/istio-basics-course:service-b-java-11
          imagePullPolicy: Always
          ports:
            - containerPort: 8082
          securityContext:
            runAsUser: 1000
          resources:
            limits:
              memory: 32Mi
              cpu: 100m
``` 
Применим манифест Service для деплоймента выше:
``` shell
kubectl apply -f producer-internal-host.yml
```
``` yaml
apiVersion: v1
kind: Service
metadata:
  name: producer-internal-host
spec:
  ports:
    - port: 80
      name: http-80
      targetPort: 8082
  selector:
    app: service-b-app
``` 
Определим правило маршрутизации запросов из ServiceA на хост producer-internal-host.

Россмотрим producer-internal-host-vs:
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: producer-internal-host-vs
spec:
  hosts:
    - producer-internal-host
  gateways:
    - mesh
  http:
    - route:
        - destination:
            host: producer-internal-host
            port:
              number: 80
``` 
Применим данный манифест
``` shell
kubectl apply -f producer-internal-host-vs.yml
``` 
Проверим готовность подов:
``` shell
kubectl get pods --all-namespaces
``` 
Повторим совершенный на предидущем шаге GET запрос по адресу ingress-шлюза:
``` shell
curl -v http://$GATEWAY_URL/service-a
``` 
В случае успеха ответ на совершенный вызов должен быть таким: Hello from ServiceA! Calling master system API... Received response from master system (http://producer-internal-host): Hello from ServiceB!

Для сравнения аналогичный вызов на предыдущем шаге возвращал такой ответ: Hello from ServiceA! Calling master system API... I/O error on GET request for "http://producer-internal-host": producer-internal-host; nested exception is java.net.UnknownHostException: producer-internal-host

Теперь в кластере существуют поставщик данных для ServiceA, который связан с хостом producer-internal-host, поэтому ServiceA на этом шаге получает корректный ответ.

Проверим логи доступа Envoy ingress-шлюза:
``` shell
kubectl logs -l app=istio-ingressgateway -n istio-system -c istio-proxy
``` 
Проверим логи доступа Envoy в поде с бизнес сервисом ServiceA:
``` shell
kubectl logs -l app=service-a-app -c istio-proxy
``` 
Проверим логи доступа Envoy в поде с бизнес сервисом ServiceB:
``` shell
kubectl logs -l app=service-b-app -c istio-proxy
``` 
Перейдем далее.

### Расщепление трафика из ServiceA.
На этом шаге мы настроим балансиоровку исходящего трафика из ServiceA на два сервсиса-поставщика данных - ServiceB и ServiceC.
![sc3](https://github.com/ArtashesAvetisyan/sbercode-scenarios/blob/master/assets/sc2-3-50.png?raw=true)
Схема service mesh, в соотвесвтии с которой будем настраивать наш кластер:

Mesh configuration

Установим ServiceC:
``` shell
kubectl apply -f service-c-deployment.yml
```
``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-c-deployment
  labels:
    app: service-c-app
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-c-app
      version: v1
  template:
    metadata:
      labels:
        app: service-c-app
        version: v1
    spec:
      containers:
        - name: service-c-container
          image: artashesavetisyan/istio-basics-course:service-c-java-11
          imagePullPolicy: Always
          ports:
            - containerPort: 8082
          securityContext:
            runAsUser: 1000
          resources:
            limits:
              memory: 32Mi
              cpu: 100m
```
Применим манифест Service для деплоймента ServiceC:
``` shell
kubectl apply -f service-c-srv.yml
```
``` yaml
apiVersion: v1
kind: Service
metadata:
  name: service-c-srv
  labels:
    app: service-c-app
    service: service-c-app-srv
spec:
  ports:
    - port: 80
      name: http-80
      targetPort: 8082
  selector:
    app: service-c-app
```
Россмотрим новую версию правила маршрутизации producer-internal-host-vs:
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: producer-internal-host-vs
spec:
  hosts:
    - producer-internal-host
  gateways:
    - mesh
  http:
    - route:
        - destination:
            host: producer-internal-host
            port:
              number: 80
          weight: 50
        - destination:
            host: service-c-srv
            port:
              number: 80
          weight: 50
``` 
Блок spec.http[0].route содержит два вложенных блока destination с хостами producer-internal-host и service-c-srv, а также с ключами weight, содержашими значания процентных долей для расщепления трафика и перенаправления всех поступивших на хост producer-internal-host (ключ spec.hosts) запросов.

Обновим вирутальный сервис producer-internal-host-vs, созданный на предидущем шаге, новым манифестом producer-internal-host-50-c-vs.yml:
``` shell
kubectl apply -f producer-internal-host-50-c-vs.yml
``` 
Теперь, приблизительно 50% запросов будут направлены на Service C, оставшиейся как и ранее - на Service B. Совершите 5-6 запрсоов и убедитесь, что в отве присутсвуют данные из разных сервисов.

curl -v http://$GATEWAY_URL/service-a
Теперь среди ответов мы увидим уже известный нам вариант: Hello from ServiceA! Calling master system API... Received response from master system (http://producer-internal-host): Hello from ServiceB!

Но будет также новый вариант:

Hello from ServiceA! Calling master system API... Received response from master system (http://producer-internal-host): Hello from ServiceC! Calling master system API... 404 Not Found: [no body]

Такой ответ - результат направления запроса из ServiceA в ServiceC, который пытается получить данные из своего поставщика http://istio-ingressgateway.istio-system.svc.cluster.local/service-ext.

При последующих вызовах ответы продрожат чередоваться, так как мы расщепили трафик на два сервиса, как отражено на схеме.
``` shell
curl -v http://$GATEWAY_URL/service-a
``` 
Перейдем далее.

### Создание виртуального кластера external-cluster
На этом шаге мы создадим новое пространство имен или виртуальный кластер, который изолирован от текущего пространства имен dev-service-mesh, и будет содержать прикладной сервис, поставляющий данные для Service C.

Весь исходящий трафик из dev-service-mesh будут направляться на egress-шлюз, который в свою очередь будет проксировать все запросы в пространство external-cluster.

Новый кластер мы будем создавать аналогично тому, как это подробно представлено в упражнении Конфигурация окружения и запуск прикладного сервиса в sevice mesh

Давайте создадим новое пространство имен (виртуальный кластер):
``` shell
kubectl create namespace external-cluster
``` 
Настроим авто-внедрение envoy-прокси в данном пространстве имен:
``` shell
kubectl label namespace external-cluster istio-injection=enabled
``` 
Развернем новый Deployment прикладного сервиса External Cluster Service:
``` shell
kubectl apply -f service-ext-deployment.yml -n external-cluster
```
``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-ext-deployment
  labels:
    app: service-ext-app
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-ext-app
      version: v1
  template:
    metadata:
      labels:
        app: service-ext-app
        version: v1
    spec:
      containers:
        - name: service-ext-container
          image: artashesavetisyan/istio-basics-course:service-ext-java-11
          imagePullPolicy: Always
          ports:
            - containerPort: 8082
          securityContext:
            runAsUser: 1000
          resources:
            limits:
              memory: 32Mi
              cpu: 100m
```

Создадим для него Service:
``` shell
kubectl apply -f service-ext-srv.yml -n external-cluster
```
``` yaml
apiVersion: v1
kind: Service
metadata:
  name: service-ext-srv
  labels:
    app: service-ext-app
    service: service-ext-app-srv
spec:
  ports:
    - port: 80
      name: http-80
      targetPort: 8082
  selector:
    app: service-ext-app
```
Откроем доступ к хосту данного сервиса через ingress-шлюз:
``` shell
kubectl apply -f service-ext-gw.yml -n external-cluster
```
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: service-ext-gw
spec:
  selector:
    istio: ingressgateway
  servers:
    - port:
        number: 80
        name: http
        protocol: HTTP
      hosts:
        - "*"
```
Настроем внутрикластерную маршрутизацию трафика из ingress-шлюза в прикладной сервис:
``` shell
kubectl apply -f inbound-to-service-ext-vs.yml -n external-cluster
```
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: inbound-to-service-ext-vs
spec:
  hosts:
    - "*"
  gateways:
    - service-ext-gw
  http:
    - match:
        - uri:
            exact: /service-ext
      rewrite:
        uri: /
      route:
        - destination:
            host: service-ext-srv
            port:
              number: 80
```

Убедимся, что все поды работают:
``` shell
kubectl get pods --all-namespaces
``` 
Проверим новый сервис обратившись к нему:
``` shell
curl -v http://$GATEWAY_URL/service-ext
``` 
Перейдем далее.

### Открытие исходящего трафика из dev-service-mesh
На данном шаге мы откроем исходящий трафик из пространсва имен dev-service-mesh для получения ответов из пространсва имен external-cluster на запросы ServiceC.

Схема service mesh, в соотвесвтии с которой будем настраивать наш кластер:
![sc4](https://github.com/ArtashesAvetisyan/sbercode-scenarios/blob/master/assets/sc2-4.png?raw=true)
Mesh configuration

Существует 3 подхода к открытию исходящего трафика в Istio:

Открытый доступ из любого пода на любой внешний хост по умолчанию - удобный подход для разработки, но не безопасный и не контролируемый, поэтому в промышленной эксплуатации применяется редко.

Отсутствие доступа на любой внешний хост исключая те, которые явно указаны в манифесте ServiceEntry.

Направление трафика на внешний хост через единый egress шлюз - позволяет обогатить весь исходящий трафик из кластера требуемой логикой (например обогатить заголовками для аутентификации запросов), мониторировать и контролировать его. Данный подход применяться в больших промышленных системах.

Реализуем третий подход.

Развернем egress-шлюз, выполнив команду авто-конфигруации Isto:
``` shell
istioctl -c /etc/rancher/k3s/k3s.yaml install -y --set components.egressGateways[0].name=istio-egressgateway --set components.egressGateways[0].enabled=true --set meshConfig.accessLogFile=/dev/stdout --set meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY --set values.pilot.resources.requests.memory=128Mi --set values.pilot.resources.requests.cpu=50m --set values.global.proxy.resources.requests.cpu=10m --set values.global.proxy.resources.requests.memory=32Mi --set values.global.proxy.resources.limits.memory=64Mi --set values.pilot.resources.limits.memory=256Mi
``` 
В случае успеха, в выводе вышеприведенной команды должны быть строки:

✔ Istio core installed                                                        
✔ Istiod installed                                                            
✔ Ingress gateways installed                                                  
✔ Egress gateways installed                                                   
✔ Installation complete
Создадим манифест Gateway для исходящего трафика:
``` shell
kubectl apply -f service-ext-outbound-gw.yml
```
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: istio-egressgateway
spec:
  selector:
    istio: egressgateway
  servers:
    - port:
        number: 80
        name: http
        protocol: HTTP
      hosts:
        - istio-ingressgateway.istio-system.svc.cluster.local
```
Рассмотрим новое правило маршрутизации:
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: outbound-srv-c-to-external-srv-vs
spec:
  hosts:
    - istio-ingressgateway.istio-system.svc.cluster.local
  gateways:
    - istio-egressgateway
    - mesh
  http:
    - match:
        - gateways:
            - mesh
          port: 80
      route:
        - destination:
            host: istio-egressgateway.istio-system.svc.cluster.local
            port:
              number: 80
    - match:
        - gateways:
            - istio-egressgateway
          port: 80
      route:
        - destination:
            host: istio-ingressgateway.istio-system.svc.cluster.local
            port:
              number: 80
``` 
В соответствии с этим манифестом новое правило будет работать при вызовах на хост istio-ingressgateway.istio-system.svc.cluster.local из шлюза istio-egressgateway, а также из любого envoy-прокси в неймспейсе. Если вызов прийдет из любого envoy-прокси в неймспейсе (кроме istio-egressgateway), произойдет его перенаправление на хост istio-egressgateway. Если поступит запрос из istio-egressgateway, то он будет направлен на хост istio-ingressgateway.istio-system.svc.cluster.local. Таким образом достигается сосредоточение всех исходящих вызовов в кластере на шлюз istio-egressgateway.

Применим это правило:
``` shell
kubectl apply -f outbound-srv-c-to-service-ext-vs.yml
```

Теперь исходящий трафик направляется через egress-шлюз и достигает istio-ingressgateway.istio-system.svc.cluster.local.

Совершим несколько запросов на ingress-шлюз, напомню, запросы из ServiceA все также балансируются между ServiceB и ServiceC:
``` shell
curl -v http://$GATEWAY_URL/service-a
``` 
На этом шаге все ответы должны быть успешные и иметь вид (если поступили в ServiceA из ServiceB): Hello from ServiceA! Calling master system API... Received response from master system (http://producer-internal-host): Hello from ServiceB!

Или поступили в ServiceA из ServiceC (см. схему сети выше):

Hello from ServiceA! Calling master system API... Received response from master system (http://producer-internal-host): Hello from ServiceC! Calling master system API... Received response from master system (http://istio-ingressgateway.istio-system.svc.cluster.local/service-ext): Hello from External Cluster Service!
Обратите внимание, что в части ответа из ServiceC присутвует ответ из кластера external-cluster по запросу http://istio-ingressgateway.istio-system.svc.cluster.local/service-ext

Если исходящий трафик планируется направить на хост, который не зарегистриован в сети (в другой сетевой контур, например, в открытом Интернете), то в том случае следует дополнительно создать манифест ServiceEntry с описанием ного хоста.

Перейдем далее.

### Перевод всех запросов из ServiceA в ServiceC
Теперь, когда мы убедились в работоспособности пути ingress-шлюз -> ServiceA -> SericeC -> external-cluster, давайте переключим 100% трафика из ServiceA в ServiceC.

Для этого нам нужно будет обновить манифест producer-internal-host-vs.

Рассмотрим новую версию:
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: producer-internal-host-vs
spec:
  hosts:
    - producer-internal-host
  gateways:
    - mesh
  http:
    - route:
        - destination:
            host: service-c-srv
            port:
              number: 80
``` 
Как видите, теперь в блоке destination присутствует только хост service-c-srv, который ведет на ServiceC. Напомню, ServiceA продолжит высылать запросы на хост producer-internal-host. Но сработает перенаправление на ServiceC, вместо ServiceB.

Применим манифест:
``` shell
kubectl apply -f producer-internal-host-100-c-vs.yml
``` 
Совершим несколько запросов на ingress-шлюз:
``` shell
curl -v http://$GATEWAY_URL/service-a
``` 
Теперь все ответы из ServiceC:
``` shell
Hello from ServiceA! Calling master system API... Receive
``` 
