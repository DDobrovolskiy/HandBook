Нужно установить истио в кластер 

![schema](https://github.com/ArtashesAvetisyan/sbercode-scenarios/blob/master/assets/sc2-1.png)

### Конфигурация Истио
``` shell 
istioctl -c /etc/rancher/k3s/k3s.yaml install -y --set meshConfig.accessLogFile=/dev/stdout --set meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY --set values.pilot.resources.requests.memory=128Mi --set values.pilot.resources.requests.cpu=50m --set values.global.proxy.resources.requests.cpu=10m --set values.global.proxy.resources.requests.memory=32Mi
✔ Istio core installed                                                                                                                                                     
✔ Istiod installed                                                                                                                                                         
✔ Ingress gateways installed                                                                                                                                               
✔ Installation complete    
```
Обратите внимание на параметры, применяемые в данной команде:

meshConfig.accessLogFile=/dev/stdout - активации записи логов доступа Envoy каждого контейнера с прокси-сервером

meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY - разрешаем прокси-сервером перенаправлять запросы на внешние хосты, которые явно указаны во внутреннем реестре Istio. В ином случае, Istio позволит совершить исходящий запрос на любой хост.

### Создание пространства имен
Мы создадим логическое изолированное пространство, где в дальнейшем будем разворачивать сервисы для этого мы будем использовать новое пространство имен (виртуальный кластер) Kubernetes.

Для создания нового пространства имен dev-service-mesh выполним команду:
``` shell 
kubectl create namespace dev-service-mesh
``` 
Получим список всех существующих пространств имен в нашем кластере и убедимся что dev-service-mesh есть в списке:
``` shell 
kubectl get namespace
``` 
Конфигурация пространства имен
Для того чтобы при написании очередной команды kubectl каждый раз не указывать то пространство имен, в котором следует исполнить команду, укажем в текущем контексте конфигураций Kubernetes созданное пространство dev-service-mesh:
``` shell 
kubectl config set-context --current --namespace=dev-service-mesh
``` 
Убедимся что dev-service-mesh применено в текущем контексте:
``` shell 
kubectl config view --minify | grep namespace:
``` 
Теперь настроим в пространсве имен dev-service-mesh автоматическое внедрение контейнера с прокси-сервером Envoy в каждый создаваемый под, содержащий контейнер с бизнес-сервисом:
``` shell 
kubectl label namespace dev-service-mesh istio-injection=enabled
```

### Запуск пода с бизнес сервисом
На этом и следующих шагах мы развернем бизнес сервис в service mesh и откроем к нему доступ из вне.

Схема создаваемой конфигурации сети:

Mesh configuration

Мы применим манифест Deploymnet на основе чего будет запущен под, содержащий бизнес-сервис.

Давайте рассмотрим этот манифест:
``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-b-deployment
  labels:
    app: service-b-app
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-b-app
      version: v1
  template:
    metadata:
      labels:
        app: service-b-app
        version: v1
    spec:
      containers:
        - name: service-b-container
          image: artashesavetisyan/istio-basics-course:service-b
          imagePullPolicy: Always
          ports:
            - containerPort: 8082
          securityContext:
            runAsUser: 1000
``` 
Обратите внимание на ключ spec.template.spec.containers[0].image, значение которого содержит ссылку на image с бизнес сервисом ServiceB из DockerHub, ключ spec.template.spec.containers[0].ports[0].containerPort (8082), содержащий номер порта который будет открыт у создаваемого контейнера и ключ metadata.labels.app, значение которго будет использовано в следующем манифесте.

ServiceB - это веб-приложение на базе Spring Boot и Java, которое принимает GET запросы по адресу http://localhost:8082/ и возвращает константный ответ вида: Hello from ServiceB!

Исходный код приложения:

https://github.com/ArtashesAvetisyan/sbercode-scenarios/tree/master/apps/ServiceB
Давайте применим манифест, выполнив команду:
``` shell 
kubectl apply -f service-b-deployment.yml
``` 
Проверим статус созданного пода:
``` shell 
kubectl get pods
```
Дождемся состояния пода вида:
``` shell 
NAME                                    READY   STATUS    RESTARTS   AGE
service-b-deployment-786dc4d5b4-h8lkm   2/2     Running   0          14s
``` 
Запишем имя созданного пода в переменную POD_NAME:
``` shell 
export POD_NAME=$(kubectl get pod -l app=service-b-app -o jsonpath="{.items[0].metadata.name}")
```
Рассмотрим логи приложения ServiceB, выполнив команду:
``` shell 
kubectl logs $POD_NAME
``` 
Рассмотрим конфигурацию и состояние пода более подробно, выполнив команду:
``` shell 
kubectl describe pod $POD_NAME
``` 
Перейдем к следующему шагу.

### Применение манифеста Service
Манифест Service позволяет Kubernetes создать некую абстракцию над коллекцией подов и открыть к ним единый адрес доступа, закрепив за ними определенный хост и IP адрес.

Давайте рассмотрим этот манифест:
``` yaml
apiVersion: v1
kind: Service
metadata:
  name: producer-internal-host
spec:
  ports:
    - port: 80
      name: http-80
      targetPort: 8082
  selector:
    app: service-b-app
``` 
Обратите внимание на значение ключа metadata.name - содержит имя хоста, по которому будет доступно внутри service mesh приложение, имя которого указно в ключе spec.selector.app.

Также следует учесть, что spec.ports[0].port (80) содержит номер порта, на котором будет доступен хост из metadata.name, но трафик далее будет направлен на порт, указанный в значении ключа spec.ports[0].targetPort.

Давайте применим этот манифест:
``` shell 
kubectl apply -f producer-internal-host.yml
``` 
Получим его детальное описание из cubectl:
``` shell 
kubectl describe service producer-internal-host
```
### Перейдем к созданию Gateway

Применение манифеста Gateway
Манифест Gateway из API Istio конфигурирует изолированный envoy-proxy, который управляет всем входящим (ingress-шлюз) или исходящим (egress-шлюз) трафиком сети.

На данном шаге мы будем конфигурировать ingres-шлюз представляющий собой под с контейнером envoy-proxy из пространства имен istio-system, где он был развернут автоматически при установке istio.

Рассмотрим манифест:
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: service-b-gw
spec:
  selector:
    istio: ingressgateway
  servers:
    - port:
        number: 80
        name: http
        protocol: HTTP
      hosts:
        - "*"
```
Обратите внимание на значения ключей spec.selector.istio - содержит значение селектора istio, таким образом определяя свое действие на под, имеющий подобный селектор (istio=ingressgateway).

Ключ spec.servers[0].port.number содержит номер порта, который будет открыт у ingress-шлюза для приема входящих запросов, а ключ spec.servers[0].hosts - имя хостов, которые могут быть запрошены.

Рассмотрим детальное описание пода istio-ingressgateway, в том числе блок Labels, содержащий среди прочего - istio=ingressgateway:
``` shell 
kubectl describe pod -l app=istio-ingressgateway -n istio-system
``` 
Давайте применим service-b-gw.yml:
``` shell 
kubectl apply -f service-b-gw.yml
```
Получим детальное описание созданного ресурса:
``` shell 
kubectl describe gateway.networking.istio.io service-b-gw
``` 
Перейдем к следующему шагу.

### Применение манифеста VirtualService
Манифест VirtualService из API Istio определяет список правил маршрутизации трафика внутри service-mesh в привязке к имени вызываемого хоста.

На данном шаге мы применим манифест определяющий те правила, которые позволят запросам из ingress-щлюза поступить в под с бизнес сервисом ServiceB.

Рассмотрим манифест:
``` yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: inbound-to-service-b-vs
spec:
  hosts:
    - "*"
  gateways:
    - service-b-gw
  http:
    - match:
        - uri:
            exact: /service-b
      rewrite:
        uri: /
      route:
        - destination:
            host: producer-internal-host
            port:
              number: 80
``` 
Обратите внимание на значения ключей spec.hosts, spec.gateways. Первый содержит список хостов, которые охватывает данное правило, второй - список имен шлюзов, запросы из которых будут учтены данным правилом (здесь указано значение имени Gateway, созданного на предыдущем шаге).

Ключ spec.http[0].match[0].uri.exact содержит значение HTTP заголовка path в запросе, он же определяет запрошенный путь, в данном случае это - "/service-b".

Ключ spec.http[0].rewrite.uri содержит то значение, на которое следует заменить значение заголовка path поступившего запроса, в данном случае это "/", то есть запросы с путем "/service-b" будут направлены на корневой каталог ("/") хоста назначения.

Ключ spec.http[0].route[0].destination.host содержит имя хоста назначения, в данном случае producer-internal-host - имя в манифесте Service, созданном на шаге 5.

Ключ spec.http[0].route[0].destination.port.number содержит значение порта упомянутого сервиса.

Давайте применим inbound-to-service-b-vs.yml:
``` shell 
kubectl apply -f inbound-to-service-b-vs.yml
``` 
Получим детальное описание созданного ресурса:
``` shell 
kubectl describe virtualservice.networking.istio.io inbound-to-service-b-vs
``` 
Перейдем к следующему шагу.

### Тестирование пути входящего трафика
На данный момент мы завершили создание манифестов, необходимых для достижения цели открытия входящего трафика в service mesh.

Для того чтобы апробировать путь входящих запросов в ServiceB, следует совершить запрос на адрес ingress-шлюза, который должен перенаправить поступивший запрос в адрес envoy-прокси в поде с бизнес сервисом, который в свою очередь направит запрос в ServecB. Этот сервис получив запрос, сформирует и отправит ответ по обратному пути.

Для начала запросим краткое описание манифеста Service ingress-шлюза, созданного при установке Istio:
``` shell 
kubectl get svc istio-ingressgateway -n istio-system
``` 
Экспортируем IP-адрес из этого ресурса в переменную:
``` shell 
export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
``` 
То же самое сделаем для номера порта ingress-шлюза:

export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
Создадим переменную, содержащую извлеченные ранее данные:
``` shell 
export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
``` 
Таким образом, переменная GATEWAY_URL содержит адрес по которому можно совершить запрос в ingress-шлюз:
``` shell 
echo $GATEWAY_URL
``` 
Перед запросом убедимся в корректности статусов всех подов всех пространств имен:
``` shell 
kubectl get pods --all-namespaces
``` 
И наконец совершим GET запрос по адресу ingress-шлюза:
``` shell 
curl -v http://$GATEWAY_URL/service-b
``` 
В случае успеха в теле ответа мы должны видеть сообщение: Hello from ServiceB!

Проверим логи доступа Envoy ingress-шлюза:
``` shell 
kubectl logs -l app=istio-ingressgateway -n istio-system -c istio-proxy
``` 
Обратите внимание на запись стандартного вывода Envoy:

[2021-09-13T20:04:15.299Z] "GET /service-b HTTP/1.1" 200 - via_upstream - "-" 0 21 125 124 "10.42.0.1" "curl/7.68.0" "9629157d-09ba-4c97-b3f4-f7a277ee055e" "10.180.179.167" "10.42.0.9:8082" outbound|80||producer-internal-host.dev-service-mesh.svc.cluster.local 10.42.0.7:60256 10.42.0.7:8080 10.42.0.1:60625 - -

В нем приведено подробное описание полученного запроса Envoy-прокси, выступающего в роли ingress-шлюза, и совершенного перенаправления этого запроса согласно правилам маршрутизации. Подробнее о стандартном выводе Envoy приведено в разделе "Телеметрия" теоретической части курса.

Проверим логи доступа Envoy в поде с бизнес сервисом:
``` shell 
kubectl logs -l app=service-b-app -c istio-proxy
``` 
